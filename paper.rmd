---
title: "Breast Cancer Diagnosis Classification Analysis"
author: Austin Lackey, Ethan Powers and Danny Laposata
date: "December 9th, 2022"
output: 
  pdf_document:
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    df_print: paged
    fig_caption: TRUE
    highlight: tango
---
```{r setup, include=FALSE}
# Import packages
library(tidyverse)
library(ggplot2)
library(GGally)
library(knitr)
library(boot)
library(caret)
library(randomForest)
library(e1071)
library(class)
set.seed(420)
# Import data
data <- read.csv("breast_cancer_diagnostic_data.csv") # Full Data
training <- read.csv("training.csv") # Training Data
test <- read.csv("test.csv")  # Test Data
# Data Cleaning
data <- data[,-1] # Remove ID column
test <- test[,-1] # Remove ID column
training <- training[,-1] # Remove ID column
# Convert diagnosis to factor
data$diagnosis <- as.factor(data$diagnosis)
test$diagnosis <- as.factor(test$diagnosis)
training$diagnosis <- as.factor(training$diagnosis)
```

\newpage

# Introduction (Should we do an Abstract?)

## Background

In our analysis, we will be using the Breast Cancer Wisconsin (Diagnostic) Data Set from Kaggle. 
This data set contains `569 observations` of breast cancer cells with `32 variables` describing each cell. 
Some of the variables include characteristics like `radius`, `texture`, `area`, `perimeter` of the cell.  

The goal of our analysis is to predict whether a cell is benign or malignant based on the `32 variables`.
A cell is considered benign if it is not cancerous and malignant if it is cancerous.
Normally in most machine learning models, we do our best to train the model to reduce the overall error rate.
While this is an important goal, our group was more concerned with the `Type-II error rate`.
By reducing the `Type-II error rate`, we can ensure that we are not making the mistake of classifying a malignant cell as benign.
This is important in the world of Oncology because if a malignant cell is classified as benign, it could lead to a patient not receiving the proper treatment.
Whereas if a benign cell is classified as malignant, the patient may be alarmed, but a false alarm is better than a missed diagnosis.

In order to achieve our goal, we conducted the following steps:

1. Data Cleaning
2. Explatory Data Analysis
3. Classification Analysis
4. Regression Analysis
5. Overall Analysis Summary

## Data

In order to properly train and test our models, we first had to split the data into a training and test set.
We decided to use a `60/40 split` for our training and test data. This allows us to allocate more data to the training set, which will allow us to train our models more effectively.
We also decided to remove the `ID` column from the data set because it was not relevant to our analysis.



## Variable Descriptions

*Insert text here*

# Explatory Data Analysis

To begin our analysis, we first wanted to get a better understanding of the data so we could properly prepare it for our models.
As you can see in Figure A below, we have more information regarding the benign cells than the malignant cells.
To be more specific, we have 357 benign cells and 212 malignant cells.
Since we are worried about the `Type-II error rate`, we would want more information regarding the malignant cells in an ideal world.
However since this is not the case, is important to note because it could lead to a bias in our model if we do not take this into account.

```{r, fig.width= 4, fig.height= 4, echo=FALSE, message=FALSE, warning=FALSE}
# Malignant vs Benign Pie Chart
pieChart <- ggplot(data, aes(x = "", fill = diagnosis)) + 
  geom_bar(width = 1) + 
  coord_polar("y", start = 0) + 
  labs(title = "Figure A: Malignant vs Benign Cell Counts", x= "", y="", fill="Diagnosis") +  
  geom_text(stat='count', aes(label=..count..), vjust=-3, size=5)
print(pieChart)
```

*Insert text here regarding the GGPairs Plot*

```{r, fig.width= 6, fig.height= 6, echo=FALSE, message=FALSE, warning=FALSE}
training[,c(1,2,12,22)] |>
  ggpairs(aes(color = diagnosis)) + labs(title = "Figure B: Pairwise Scatterplots of Features")
```

\newpage

# Classification Analysis

During our classification analysis, we attacked `Type-II error` in two different ways.
The first thing we did was use different paramters for each model and to figure out which parameters yield the lowest `Type-II error rate`. 
However, testing many different parameters can be time consuming and computationally expensive. 
We also did a different approach by tuning his models to be more sensitive to Malignant cells. 
This allows us to reduce the `Type-II error rate` by classifying more cells as Malignant. 
However, on the downside, this also increases the `Type-I error rate` by classifying more cells as Malignant. 
By harnessing both approaches, we were able to reduce the `Type-II error rate` to its fullest while also maintaining a good `Type-I error rate`.

## Austin's Classification Proccess and Analysis

```{r load_data, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# Load the data
set.seed(420)
data <- read.csv("breast_cancer_diagnostic_data.csv") # Full Data
training <- read.csv("training.csv") # Training Data
test <- read.csv("test.csv")  # Test Data
# Data Cleaning
data <- data[,-1] # Remove ID column
test <- test[,-1] # Remove ID column
training <- training[,-1] # Remove ID column
# Convert diagnosis to factor
data$diagnosis <- as.factor(data$diagnosis)
test$diagnosis <- as.factor(test$diagnosis)
training$diagnosis <- as.factor(training$diagnosis)
```

```{r train_setup, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
train_control <- trainControl(method = "repeatedcv", summaryFunction = defaultSummary, classProbs = TRUE, number = 10, repeats = 10)
```

The following models used were tuned using a `10-fold cross validation` method that was repeated 10 times. 
Cross validation is method that is used to train a model on a subset of the data and then test the model on the remaining data. 
This process is repeated `10` times with each subset of data being used as the test set once. 
By using this method we are able to effectively train our model while also testing it on data that it has not seen before.

### KNN Model

The first model that we used was a `K Nearest Neighbor's model`. In order to tune this model, we used the `tuneGrid` parameter to test different values of `k`.
We tested `100` values of `k` ranging from `1` to `100` and then plotted the `Type-II error rate` (in purple) as well as the overall error rate (in black) for each value of `k` as shown in Figure C below.
The model with the lowest `Type-II error rate` was the model with `k = 10`. Any value of `k` greater than `10` resulted in a higher `Type-II error rate` as well as a higher overall error rate.
This can be attributed to the fact that the model is overfitting the data.

```{r knn_tuning, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
type_two_error_knn <- data.frame(k = seq(1, 100, 1), error = NA, typetwoerror = NA)

for (i in 1:nrow(type_two_error_knn)) {
    knn <- train(diagnosis ~ ., data = training, method = "knn", trControl = train_control, metric = "Accuracy", tuneGrid = data.frame(k = type_two_error_knn$k[i]))
    knn_pred <- predict(knn, test)
    knn_confusion <- confusionMatrix(knn_pred, test$diagnosis, positive = "M")
    type_two_error_knn$typetwoerror[i] <- knn_confusion$table[1, 2]/sum(knn_confusion$table)
    type_two_error_knn$error[i] <- (knn_confusion$table[1, 2] + knn_confusion$table[2, 1])/sum(knn_confusion$table)
}
print(which.min(type_two_error_knn$typetwoerror))
knn1_k <- which.min(type_two_error_knn$typetwoerror)
knn_tte <- min(type_two_error_knn$typetwoerror)
```

```{r knn_tuning_plot, echo=FALSE, message=FALSE, warning=FALSE}
ggplot() + geom_line(aes(x=type_two_error_knn$k, y=type_two_error_knn$error), size=1) + 
  geom_line(aes(x=type_two_error_knn$k, y=type_two_error_knn$typetwoerror), size=1.5, color="#c82798") + 
  labs(title = "Figure C: KNN Type II and Overall Error", x = "K", y = "Error")
```

### Tuned KNN Model

```{r tuned_knn, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(420)
tuned_knn <- train(diagnosis ~ ., data = training, method = "knn", trControl = train_control, metric = "Accuracy", tuneGrid = data.frame(k = which.min(type_two_error_knn$typetwoerror)))
knn_pred <- predict(tuned_knn, test)
knn_confusion <- confusionMatrix(knn_pred, test$diagnosis, positive = "M")
```


Using the information from the previous plot, we were able to tune our model to have a `k` value of `10`. 
This tuned model resulted in a `Type-II error rate` of `r knn_tte` and an overall accuracy of `r knn_confusion$overall[1]`.
The confusion matrix for this model is shown in Figure D below. 
As you can see out of the total `r dim(training)[1]` training samples, `r knn_confusion$table[1, 2]` were misclassified as benign when they were actually malignant.

```{r tuned_knn_plot, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data.frame(knn_confusion$table), aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Figure D: KNN Confusion Matrix", x = "Prediction", y = "Reference") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798")
```

### Random Forrest Model

```{r rf_tuning, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
type_two_error_rf <- data.frame(ntree = seq(1, 100, 10), error = NA, typetwoerror = NA)

for (i in 1:nrow(type_two_error_rf)) {
    rf <- train(diagnosis ~ ., data = training, method = "rf", trControl = train_control, metric = "Accuracy", ntree = type_two_error_rf$ntree[i])
    rf_pred <- predict(rf, test)
    rf_confusion <- confusionMatrix(rf_pred, test$diagnosis, positive = "M")
    type_two_error_rf$typetwoerror[i] <- rf_confusion$table[1, 2]/sum(rf_confusion$table)
    type_two_error_rf$error[i] <- (rf_confusion$table[1, 2] + rf_confusion$table[2, 1])/sum(rf_confusion$table)
}
print(which.min(type_two_error_rf$typetwoerror))
rf1_ntree <- type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)]
rf_tte <- min(type_two_error_rf$typetwoerror)
```

The second model that we used was a `Random Forrest model`. In order to tune this model, we used the `tuneGrid` parameter to test different numbers of `trees`.
We tested `10` values of `trees` ranging from `1` to `100` and then plotted the `Type-II error rate` (in purple) as well as the overall error rate (in black) for each value of `trees` as shown in Figure E below. 
The model with the lowest `Type-II error rate` was the model with `trees =` `r type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)]`. 
Any value of `trees` greater than `r type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)]` resulted in a higher `Type-II error rate` or a higher computation time for the same `Type-II error rate`.


```{r rf_tuning_plot, echo=FALSE, message=FALSE, warning=FALSE}
ggplot() + 
  geom_line(aes(x=type_two_error_rf$ntree, y=type_two_error_rf$error), size=1) + 
  geom_line(aes(x=type_two_error_rf$ntree, y=type_two_error_rf$typetwoerror), size=1.5, color="#c82798") + 
  labs(title = "Figure E: Random Forrest Type II and Overall Error", x = "Number of Trees", y = "Error")
```

### Tuned Random Forrest

Using the information from the previous plot, we were able to tune our model to have a `trees` value of `r type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)]`.
This tuned model resulted in a `Type-II error rate` of `r rf_tte` and an overall accuracy of `r rf_confusion$overall[1]`.
The confusion matrix for this model is shown in Figure F below.
As you can see out of the total `r dim(training)[1]` training samples, `r rf_confusion$table[1, 2]` were misclassified as benign when they were actually malignant.
And we were able to further reduce our `Type-II error rate` from `r knn_tte` to `r rf_tte` when compared to the KNN model.

```{r tuned_rf, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
tuned_rf <- train(diagnosis ~ ., data = training, method = "rf", trControl = train_control, metric = "ROC", ntree = type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)])
rf_pred_tuned <- predict(tuned_rf, test)
rf_confusion <- confusionMatrix(rf_pred_tuned, test$diagnosis, positive = "M")
```

```{r tuned_rf_plot, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data.frame(rf_confusion$table), aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Figure F: Random Forrest Confusion Matrix", x = "Prediction", y = "Reference") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798")
```

### Radial Support Vector Machine

```{r austin_svm, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
#Radial Support Vector Machine
svm <- train(diagnosis ~ ., data = training, method = "svmRadial", trControl = train_control, metric="ROC")
svm_pred <- predict(svm, test)
svm_confusion <- confusionMatrix(svm_pred, test$diagnosis, positive="M")

svm_tte <- svm_confusion$table[1, 2]/sum(svm_confusion$table)
```

The third model that we used was a `Radial Support Vector Machine`.
To begin we used a basic model with the default parameters and then tuned the model using the `tuneGrid` parameter.
The basic SVM model resulted in a `Type-II error rate` of `r svm_tte` and an overall accuracy of `r svm_confusion$overall[1]`.
The confusion matrix for this model is shown in Figure G below.
As you can see out of the total `r dim(training)[1]` training samples, `r svm_confusion$table[1, 2]` was misclassified as benign when they it was actually malignant.
This is a great improvement in our `Type-II error rate` when compared to the other models.


```{r austin_svm_plot, echo=FALSE, message=FALSE, warning=FALSE}

ggplot(data.frame(svm_confusion$table), aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Figure G: Radial SVM Confusion Matrix", x = "Prediction", y = "Reference") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798")
```


### Tuned Radial Support Vector Machine

```{r austin_svm2, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
tuned_svm <- tune.svm(diagnosis ~., data = training, gamma = 10^(-5:-1), cost = 10^(-3:1))
svm2 <- svm(diagnosis ~., data = training, gamma = 0.01, cost = 10, type="C-classification", kernel="radial")
svm2_pred <- predict(svm2, test)
svm2_confusion <- confusionMatrix(svm2_pred, test$diagnosis, positive="M")

svm2_tte <- svm2_confusion$table[1, 2]/sum(svm2_confusion$table)

```

The final model that we used was a tuned `Radial Support Vector Machine`.
The tuned model resulted in a `Type-II error rate` of `r svm2_tte` and an overall accuracy of `r svm2_confusion$overall[1]`.
The confusion matrix for this model is shown in Figure H below.
Out of the total `r dim(training)[1]` training samples, `r svm2_confusion$table[1, 2]` were misclassified as benign when they were actually malignant.
As you can see, our `Type-II error rate` was actually increased to `r svm2_tte` when compared to the basic SVM model.
Since this svm model was tuned using carets `tune.svm` function, the overall error was reduced, but this resulted in a slightly higher `Type-II error rate`.
For this reason, our basic model is actually better than the tuned model when it comes to meeting our goal of reducing the `Type-II error rate`.


```{r austin_svm_plot2, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data.frame(svm2_confusion$table), aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Figure H: Tuned Radial SVM Confusion Matrix", x = "Prediction", y = "Reference") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798")
```

\newpage

## Dannys's Classification Proccess and Analysis

```{r, echo = F, warning = F, message = F}
library(tidyverse)
library(GGally)
library(ggplot2)
library(tidyr)
library(tidymodels)
library(vip)
library(caTools)
library(xgboost)
library(discrim)
library(glmnet)
library(kableExtra)

### Gathering Data

train <- read.csv("training.csv")
test <- read.csv("test.csv")

train |>
  select(-id) -> train

test |>
  select(-id) -> test

train$diagnosis <- as.factor(train$diagnosis)
test$diagnosis <- as.factor(test$diagnosis)
```

As the main purpose of this analysis is to reduce Type II error, we wanted to try different ways of recuding this within similar models. The following models all use classification to classify a tumor as benign or malignant however they have a lower prediction cutoff of 25% malignant to bias our results towards more malignant. For example, if a tumor is only 30% likely to be malignant, we still would classify that tumor as malignant to avoid false negatives. While this does lead to higher Type 1 error and sometimes lower overall accuracy, we aren't as worried about the false-positives. We fit a number of models on the training data using this approach, including Logistic Regression, K-Nearest Neighbors, Quadratic Discriminant Analysis (QDA), Random Forest, xgBoost, and lastly a support vector classifier. While all the most models performed quite decently, there were a few that stood out.

### K-Nearest Neighbors

```{r, echo = F, warning = F, message = F}
set.seed(445)
### KNN

cancer_10foldcv <- vfold_cv(train, v = 10)
neighbors_df <- data.frame(neighbors = c(1,3,5,7,10))

knn_spec <- the_model <- nearest_neighbor(neighbors = tune("neighbors")) %>%
  set_engine("kknn") %>% 
  set_mode("classification") 

knn_rec <- recipe(diagnosis ~ ., data = train)

workflow() |>
  add_model(knn_spec) |>
  add_recipe(knn_rec) -> knn_wf

knn_wf |>
  tune_grid(resamples = cancer_10foldcv, grid = neighbors_df) -> knn_tune

knn_final <- finalize_workflow(knn_wf, select_best(knn_tune, metric = "accuracy"))
knn_final_fit <- fit(knn_final, data = train)

knn_final_fit |>
  augment(new_data = test) -> knn_test_res

knn_test_res |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  conf_mat(truth = diagnosis, estimate = pred_lower_cutoff) -> knn.cm

knn_test_res |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  accuracy(truth = diagnosis, estimate = pred_lower_cutoff) |>
  mutate(error = 1 - .estimate) |>
  pull(error) -> knn.err

100 - (round(knn.err, 4) * 100) -> knn.err

neighbors_best <- select_best(knn_tune, metric = "accuracy") |> pull(neighbors)
```

K-Nearest Neighbors is a classifying method that estimates the bayes classifier using the closest training points to the test point that is being classified. It uses a neighbor parameter that is a big factor in determining the classifier. For this model, we used 10-fold cross-validation to tune the neighbor parameter based on the highest accuracy. Once the neighbors parameter was tuned to `r neighbors_best`, the model was fit and prediction ensued. The confusion matrix showcasing the classification prediction of this model is below. 

```{r, echo = F, warning = F, message = F}
ggplot(data.frame(knn.cm$table), aes(x = Prediction, y = Truth, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 12) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "K-Nearest Neighbors Confusion Matrix", x = "Prediction", y = "Truth") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798") +
  guides(fill = F)
```

As shown in the confusion matrix, this KNN model only predicted two false-negatives out of 223 test observations and 90 true test malignant tumors. The KNN model predicted with an overall accuracy of `r knn.err`. While this model did perform quite well, we still want to improve these results.

### Support Vector Classifier

```{r, echo = F, warning = F, message = F}
set.seed(445)
### SVC

df_cost <- grid_regular(cost(), levels = 10)

svm_linear_tune_spec <- svm_poly(degree = 1, cost = tune("cost")) %>%
  set_mode("classification") %>%
  set_engine("kernlab", scaled = FALSE)

svm_linear_rec <- recipe(diagnosis ~ ., data = train)

svm_linear_wf <- workflow() |>
  add_model(svm_linear_tune_spec) |>
  add_recipe(svm_linear_rec)

tune_fit <- svm_linear_wf |>
  tune_grid(resamples = cancer_10foldcv, grid = df_cost)

svm_linear_final <- finalize_workflow(svm_linear_wf, select_best(tune_fit, metric = "accuracy"))

svm_linear_final |>
  fit(data = train) -> svm_linear_final_fit

svm_linear_final_fit |>
  extract_fit_engine() -> svm_linear_final_fit_engine

svm_linear_final_fit |>
  augment(new_data = test) |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  conf_mat(truth = diagnosis, estimate = pred_lower_cutoff) -> svm.cm

svm_linear_final_fit |>
  augment(new_data = test) |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  accuracy(truth = diagnosis, estimate = pred_lower_cutoff) |>
  mutate(error = 1 - .estimate) |>
  pull(error) -> svm.err

100 - (round(svm.err, 4) * 100) -> svm.err

cost_best <- select_best(tune_fit, metric = "accuracy") |> pull(cost)
```

A support vector classifier uses a softened margin classifier to classify the observations but with less sensitivity than a maximal margin classifier. The support vector classifier will not necessarily classify all training points accurately, but this allows for better test prediction. We tuned our support vector classifier to take a cost value of `r cost_best` using 10-fold cross-validation. The confusion matrix showcasing the prediciton accuracy of the support vector classifier model can be seen below.

```{r, echo = F, warning = F, message = F}
ggplot(data.frame(svm.cm$table), aes(x = Prediction, y = Truth, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 12) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Support Vector Classifier Confusion Matrix", x = "Prediction", y = "Truth") +
  theme_minimal() + 
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798") +  
  guides(fill = F)
```

As shown above, the support vector classifier predicted the exact same as the KNN model. With only 2 false negatives and an overall accuracy of `r svm.err`, this model was not an improvement from the KNN model, but it is reassuring to see consistent high accuracy amongst different models. The last model we will look at is a Random Forest model.

### Random Forest

```{r, echo = F, warning = F, message = F}
### Random Forest
set.seed(420)
rf_spec <- rand_forest(mtry = sqrt(.cols())) |>
  set_engine("randomForest", importance = TRUE) |>
  set_mode("classification")

rf_fit <- rf_spec |>
  fit(diagnosis ~ ., data = train)

rf_fit |>
  augment(new_data = test) |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  conf_mat(truth = diagnosis, estimate = pred_lower_cutoff) -> rf.cm

rf_fit |>
  augment(new_data = test) |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  accuracy(truth = diagnosis, estimate = pred_lower_cutoff) |>
  mutate(error = 1 - .estimate) |>
  pull(error) -> rf.err

100 - (round(rf.err, 4) * 100) -> rf.err
```

Random Forest creates many decision trees from bootstrapped sample training points and uses a subset of predictors for each tree. The main difference between Random Forest and bagging decision trees is this feature subset that helps decorrelate the trees. For this Random Forest model, a default of 500 trees was used as tuning did not improve the model. The confusion matrix showcasing the prediction accuracy of the Random Forest model is shown below. 

```{r, echo = F, echo = F, warning = F, message = F}
ggplot(data.frame(rf.cm$table), aes(x = Prediction, y = Truth, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 12) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Random Forest Confusion Matrix", x = "Prediction", y = "Truth") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798") +  
  guides(fill = F)
```

As shown above, the Random Forest model predicted the best out of all our models according to the standards we set. With only one false-negative, this is the lowest out of all the models and with an overall accuracy of `r rf.err` it holds up against the other models and does not gain too much Type I error. This confusion matrix portrays the predicition cutoff very well. Because the model biases towards classifying as malignant, there is much higher Type I error than Type II error, which is exactly what we were aiming for.

\newpage

# Ethans's Regression Proccess and Analysis

*Insert text here*


\newpage

# Analysis Summary

*Insert text here*

# Potential Improvements

Following our analysis, we have identified a few potential improvements that could be made to our analysis to further improve our models.
The obvious and first improvement that could be made is to **collect more data**. With more data, we would allow our models to see more examples of malignant cells and be able to better classify them.
As you saw in the Exploratory Data Analysis section, we have more more information regarding the benign cells than the malignant cells. 
If we had more data, we could balance out the data set and have more information regarding the malignant cells.
Another possible improvement is to **collect different or more features** regarding each cell. 
Most of the features included the mean, worst and standard error of geometry measurements of the cell. This leads to many variables being highly correlated with each other and could lead to multicollinearity.
If we had more features, we could reduce the multicollinearity and diversify the information we have regarding each cell.
Finally, if we had **access to better technology** and more time or money, we could use a more advanced machine learning model.
Because this project was done for a class, we were limited to the models we could use as there was a time contraint, especially in a team enviroment.

\newpage

# Works Cited

- Breast Cancer Wisconsin (Diagnostic) Data Set:
    - https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data?datasetId=180&sortBy=voteCount
- Definition of Features (Variables):
    - https://www.causeweb.org/usproc/sites/default/files/usclap/2017-2/Evaluating_Benign_and_Malignant_Breast_Cancer_Cells_from_Fine-Needle_Aspirates.pdf