---
title: "Breast Cancer Diagnosis Classification Analysis"
author: Austin Lackey, Ethan Powers and Danny Laposata
date: "December 9th, 2022"
output: 
  pdf_document:
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    df_print: paged
    fig_caption: TRUE
    highlight: tango
---
```{r setup, include=FALSE}
# Import packages
library(tidyverse)
library(ggplot2)
library(GGally)
library(knitr)
library(boot)
library(caret)
library(randomForest)
library(e1071)
library(class)
set.seed(420)
# Import data
data <- read.csv("breast_cancer_diagnostic_data.csv") # Full Data
training <- read.csv("training.csv") # Training Data
test <- read.csv("test.csv")  # Test Data
# Data Cleaning
data <- data[,-1] # Remove ID column
test <- test[,-1] # Remove ID column
training <- training[,-1] # Remove ID column
# Convert diagnosis to factor
data$diagnosis <- as.factor(data$diagnosis)
test$diagnosis <- as.factor(test$diagnosis)
training$diagnosis <- as.factor(training$diagnosis)
```

\newpage

# Introduction (Should we do an Abstract?)

## Background

In our analysis, we will be using the Breast Cancer Wisconsin (Diagnostic) Data Set from Kaggle. 
This data set contains `569 observations` of breast cancer cells with `32 variables` describing each cell. 
Some of the variables include characteristics like `radius`, `texture`, `area`, `perimeter` of the cell.  

The goal of our analysis is to predict whether a cell is benign or malignant based on the `32 variables`.
A cell is considered benign if it is not cancerous and malignant if it is cancerous.
Normally in most machine learning models, we do our best to train the model to reduce the overall error rate.
While this is an important goal, our group was more concerned with the `Type-II error rate`.
By reducing the `Type-II error rate`, we can ensure that we are not making the mistake of classifying a malignant cell as benign.
This is important in the world of Oncology because if a malignant cell is classified as benign, it could lead to a patient not receiving the proper treatment.
Whereas if a benign cell is classified as malignant, the patient may be alarmed, but a false alarm is better than a missed diagnosis.

In order to achieve our goal, we conducted the following steps:

1. Data Cleaning
2. Explatory Data Analysis
3. Classification Analysis
4. Regression Analysis
5. Overall Analysis Summary

## Data

In order to properly train and test our models, we first had to split the data into a training and test set.
We decided to use a `60/40 split` for our training and test data. This allows us to allocate more data to the training set, which will allow us to train our models more effectively.
We also decided to remove the `ID` column from the data set because it was not relevant to our analysis.

## Variable Descriptions

After removing the `ID` column, we were left with `31 variables`. 
For this project our response variable is `diagnosis` as this variable tells whether an observation is Malignant (cancerous cells): `M` or Benign (noncancerous cells): `B`.
The predictor variables make up the remaining 30 columns; however there are only actually 10 variables that each have 3 measurements.
These measurements are `mean`, `standard error (SE)`, and `worst` (the mean of the 3 largest values of a variables).

Below is a brief explanation of what each of the 10 variables represent:

- Radius: Average Distance from cell center to cell perimeter
- Texture: Standard deviation of gray-scale values; brightness of pixel of cell
- Perimeter: Distance around nucleus boundary
- Area: Area of the nucleus
- Smoothness: Variation in cell's radial lengths
- Compactness: The Perimeter^2/Area
- Concavity: Size of the indention in nucleus boundary
- Concave Points: Number of points on indented section of nucleus boundary
- Symmetry: Deviation of the nuclei shape from the ideal measurement
- Fractal Dimension: Measurement of irregularity in nucleus boundary

# Explatory Data Analysis

To begin our analysis, we first wanted to get a better understanding of the data so we could properly prepare it for our models.
As you can see in Figure A below, we have more information regarding the benign cells than the malignant cells.
To be more specific, we have 357 benign cells and 212 malignant cells.
Since we are worried about the `Type-II error rate`, we would want more information regarding the malignant cells in an ideal world.
However since this is not the case, is important to note because it could lead to a bias in our model if we do not take this into account.

```{r, fig.width= 4, fig.height= 4, echo=FALSE, message=FALSE, warning=FALSE}
# Malignant vs Benign Pie Chart
pieChart <- ggplot(data, aes(x = "", fill = diagnosis)) + 
  geom_bar(width = 1) + 
  coord_polar("y", start = 0) + 
  labs(title = "Figure A: Malignant vs Benign Cell Counts", x= "", y="", fill="Diagnosis") +  
  geom_text(stat='count', aes(label=..count..), vjust=-3, size=5)
print(pieChart)
```

Another important thing to note about the data is the variables included. There are three different measurements for the ten predictor variables: mean value, standard error, and "worst" value (mean of 3 largest measurements). As shown the correlation plot below, these three measurements are all correlated to each other, but still hold a strong relationship with the diagnosis response variable. The relation between the mean value and "worst" value is almost completely correlated which makes sense as both are averages however, the "worst" value only uses the three largest measurements. This is important to know when trying to interpret feature effect, importance and selection.

```{r, fig.width= 6, fig.height= 6, echo=FALSE, message=FALSE, warning=FALSE}
training[,c(1,2,12,22)] |>
  ggpairs(aes(color = diagnosis)) + labs(title = "Figure B: Pairwise Scatterplots of Features")
```

\newpage

# Classification Analysis

During our classification analysis, we attacked `Type-II error` in two different ways.
The first thing we did was use different paramters for each model and to figure out which parameters yield the lowest `Type-II error rate`. 
However, testing many different parameters can be time consuming and computationally expensive. 
We also did a different approach by tuning his models to be more sensitive to Malignant cells. 
This allows us to reduce the `Type-II error rate` by classifying more cells as Malignant. 
However, on the downside, this also increases the `Type-I error rate` by classifying more cells as Malignant. 
By harnessing both approaches, we were able to reduce the `Type-II error rate` to its fullest while also maintaining a good `Type-I error rate`.

## Austin's Classification Proccess and Analysis

```{r load_data, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# Load the data
set.seed(420)
data <- read.csv("breast_cancer_diagnostic_data.csv") # Full Data
training <- read.csv("training.csv") # Training Data
test <- read.csv("test.csv")  # Test Data
# Data Cleaning
data <- data[,-1] # Remove ID column
test <- test[,-1] # Remove ID column
training <- training[,-1] # Remove ID column
# Convert diagnosis to factor
data$diagnosis <- as.factor(data$diagnosis)
test$diagnosis <- as.factor(test$diagnosis)
training$diagnosis <- as.factor(training$diagnosis)
```

```{r train_setup, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
train_control <- trainControl(method = "repeatedcv", summaryFunction = defaultSummary, classProbs = TRUE, number = 10, repeats = 10)
```

The following models used were tuned using a `10-fold cross validation` method that was repeated 10 times. 
Cross validation is method that is used to train a model on a subset of the data and then test the model on the remaining data. 
This process is repeated `10` times with each subset of data being used as the test set once. 
By using this method we are able to effectively train our model while also testing it on data that it has not seen before.

### KNN Model

The first model that we used was a `K Nearest Neighbor's model`. In order to tune this model, we used the `tuneGrid` parameter to test different values of `k`.
We tested `100` values of `k` ranging from `1` to `100` and then plotted the `Type-II error rate` (in purple) as well as the overall error rate (in black) for each value of `k` as shown in Figure C below.
The model with the lowest `Type-II error rate` was the model with `k = 10`. Any value of `k` greater than `10` resulted in a higher `Type-II error rate` as well as a higher overall error rate.
This can be attributed to the fact that the model is overfitting the data.

```{r knn_tuning, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
type_two_error_knn <- data.frame(k = seq(1, 100, 1), error = NA, typetwoerror = NA)

for (i in 1:nrow(type_two_error_knn)) {
    knn <- train(diagnosis ~ ., data = training, method = "knn", trControl = train_control, metric = "Accuracy", tuneGrid = data.frame(k = type_two_error_knn$k[i]))
    knn_pred <- predict(knn, test)
    knn_confusion <- confusionMatrix(knn_pred, test$diagnosis, positive = "M")
    type_two_error_knn$typetwoerror[i] <- knn_confusion$table[1, 2]/sum(knn_confusion$table)
    type_two_error_knn$error[i] <- (knn_confusion$table[1, 2] + knn_confusion$table[2, 1])/sum(knn_confusion$table)
}
print(which.min(type_two_error_knn$typetwoerror))
knn1_k <- which.min(type_two_error_knn$typetwoerror)
knn_tte <- min(type_two_error_knn$typetwoerror)
```

```{r knn_tuning_plot, echo=FALSE, message=FALSE, warning=FALSE}
ggplot() + geom_line(aes(x=type_two_error_knn$k, y=type_two_error_knn$error), size=1) + 
  geom_line(aes(x=type_two_error_knn$k, y=type_two_error_knn$typetwoerror), size=1.5, color="#c82798") + 
  labs(title = "Figure C: KNN Type II and Overall Error", x = "K", y = "Error")
```

### Tuned KNN Model

```{r tuned_knn, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(420)
tuned_knn <- train(diagnosis ~ ., data = training, method = "knn", trControl = train_control, metric = "Accuracy", tuneGrid = data.frame(k = which.min(type_two_error_knn$typetwoerror)))
knn_pred <- predict(tuned_knn, test)
knn_confusion <- confusionMatrix(knn_pred, test$diagnosis, positive = "M")
```


Using the information from the previous plot, we were able to tune our model to have a `k` value of `10`. 
This tuned model resulted in a `Type-II error rate` of `r knn_tte` and an overall accuracy of `r knn_confusion$overall[1]`.
The confusion matrix for this model is shown in Figure D below. 
As you can see out of the total `r dim(training)[1]` training samples, `r knn_confusion$table[1, 2]` were misclassified as benign when they were actually malignant.

```{r tuned_knn_plot, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data.frame(knn_confusion$table), aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Figure D: KNN Confusion Matrix", x = "Prediction", y = "Reference") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798")
```

### Random Forrest Model

```{r rf_tuning, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
type_two_error_rf <- data.frame(ntree = seq(1, 100, 10), error = NA, typetwoerror = NA)

for (i in 1:nrow(type_two_error_rf)) {
    rf <- train(diagnosis ~ ., data = training, method = "rf", trControl = train_control, metric = "Accuracy", ntree = type_two_error_rf$ntree[i])
    rf_pred <- predict(rf, test)
    rf_confusion <- confusionMatrix(rf_pred, test$diagnosis, positive = "M")
    type_two_error_rf$typetwoerror[i] <- rf_confusion$table[1, 2]/sum(rf_confusion$table)
    type_two_error_rf$error[i] <- (rf_confusion$table[1, 2] + rf_confusion$table[2, 1])/sum(rf_confusion$table)
}
print(which.min(type_two_error_rf$typetwoerror))
rf1_ntree <- type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)]
rf_tte <- min(type_two_error_rf$typetwoerror)
```

The second model that we used was a `Random Forrest model`. In order to tune this model, we used the `tuneGrid` parameter to test different numbers of `trees`.
We tested `10` values of `trees` ranging from `1` to `100` and then plotted the `Type-II error rate` (in purple) as well as the overall error rate (in black) for each value of `trees` as shown in Figure E below. 
The model with the lowest `Type-II error rate` was the model with `trees =` `r type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)]`. 
Any value of `trees` greater than `r type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)]` resulted in a higher `Type-II error rate` or a higher computation time for the same `Type-II error rate`.


```{r rf_tuning_plot, echo=FALSE, message=FALSE, warning=FALSE}
ggplot() + 
  geom_line(aes(x=type_two_error_rf$ntree, y=type_two_error_rf$error), size=1) + 
  geom_line(aes(x=type_two_error_rf$ntree, y=type_two_error_rf$typetwoerror), size=1.5, color="#c82798") + 
  labs(title = "Figure E: Random Forrest Type II and Overall Error", x = "Number of Trees", y = "Error")
```

### Tuned Random Forrest

Using the information from the previous plot, we were able to tune our model to have a `trees` value of `r type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)]`.
This tuned model resulted in a `Type-II error rate` of `r rf_tte` and an overall accuracy of `r rf_confusion$overall[1]`.
The confusion matrix for this model is shown in Figure F below.
As you can see out of the total `r dim(training)[1]` training samples, `r rf_confusion$table[1, 2]` were misclassified as benign when they were actually malignant.
And we were able to further reduce our `Type-II error rate` from `r knn_tte` to `r rf_tte` when compared to the KNN model.

```{r tuned_rf, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
tuned_rf <- train(diagnosis ~ ., data = training, method = "rf", trControl = train_control, metric = "ROC", ntree = type_two_error_rf$ntree[which.min(type_two_error_rf$typetwoerror)])
rf_pred_tuned <- predict(tuned_rf, test)
rf_confusion <- confusionMatrix(rf_pred_tuned, test$diagnosis, positive = "M")
```

```{r tuned_rf_plot, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data.frame(rf_confusion$table), aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Figure F: Random Forrest Confusion Matrix", x = "Prediction", y = "Reference") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798")
```

### Radial Support Vector Machine

```{r austin_svm, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
#Radial Support Vector Machine
svm <- train(diagnosis ~ ., data = training, method = "svmRadial", trControl = train_control, metric="ROC")
svm_pred <- predict(svm, test)
svm_confusion <- confusionMatrix(svm_pred, test$diagnosis, positive="M")

svm_tte <- svm_confusion$table[1, 2]/sum(svm_confusion$table)
```

The third model that we used was a `Radial Support Vector Machine`.
To begin we used a basic model with the default parameters and then tuned the model using the `tuneGrid` parameter.
The basic SVM model resulted in a `Type-II error rate` of `r svm_tte` and an overall accuracy of `r svm_confusion$overall[1]`.
The confusion matrix for this model is shown in Figure G below.
As you can see out of the total `r dim(training)[1]` training samples, `r svm_confusion$table[1, 2]` was misclassified as benign when they it was actually malignant.
This is a great improvement in our `Type-II error rate` when compared to the other models.


```{r austin_svm_plot, echo=FALSE, message=FALSE, warning=FALSE}

ggplot(data.frame(svm_confusion$table), aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Figure G: Radial SVM Confusion Matrix", x = "Prediction", y = "Reference") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798")
```


### Tuned Radial Support Vector Machine

```{r austin_svm2, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
tuned_svm <- tune.svm(diagnosis ~., data = training, gamma = 10^(-5:-1), cost = 10^(-3:1))
svm2 <- svm(diagnosis ~., data = training, gamma = 0.01, cost = 10, type="C-classification", kernel="radial")
svm2_pred <- predict(svm2, test)
svm2_confusion <- confusionMatrix(svm2_pred, test$diagnosis, positive="M")

svm2_tte <- svm2_confusion$table[1, 2]/sum(svm2_confusion$table)

```

The final model that we used was a tuned `Radial Support Vector Machine`.
The tuned model resulted in a `Type-II error rate` of `r svm2_tte` and an overall accuracy of `r svm2_confusion$overall[1]`.
The confusion matrix for this model is shown in Figure H below.
Out of the total `r dim(training)[1]` training samples, `r svm2_confusion$table[1, 2]` were misclassified as benign when they were actually malignant.
As you can see, our `Type-II error rate` was actually increased to `r svm2_tte` when compared to the basic SVM model.
Since this svm model was tuned using carets `tune.svm` function, the overall error was reduced, but this resulted in a slightly higher `Type-II error rate`.
For this reason, our basic model is actually better than the tuned model when it comes to meeting our goal of reducing the `Type-II error rate`.


```{r austin_svm_plot2, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(data.frame(svm2_confusion$table), aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Figure H: Tuned Radial SVM Confusion Matrix", x = "Prediction", y = "Reference") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798")
```

\newpage

## Dannys's Classification Proccess and Analysis

```{r, echo = F, warning = F, message = F}
library(tidyverse)
library(GGally)
library(ggplot2)
library(tidyr)
library(tidymodels)
library(vip)
library(caTools)
library(xgboost)
library(discrim)
library(glmnet)
library(kableExtra)

### Gathering Data

train <- read.csv("training.csv")
test <- read.csv("test.csv")

train |>
  select(-id) -> train

test |>
  select(-id) -> test

train$diagnosis <- as.factor(train$diagnosis)
test$diagnosis <- as.factor(test$diagnosis)
```

As the main purpose of this analysis is to reduce Type II error, we wanted to try different ways of recuding this within similar models. The following models all use classification to classify a tumor as benign or malignant however they have a lower prediction cutoff of 25% malignant to bias our results towards more malignant. For example, if a tumor is only 30% likely to be malignant, we still would classify that tumor as malignant to avoid false negatives. While this does lead to higher Type 1 error and sometimes lower overall accuracy, we aren't as worried about the false-positives. We fit a number of models on the training data using this approach, including Logistic Regression, K-Nearest Neighbors, Quadratic Discriminant Analysis (QDA), Random Forest, xgBoost, and lastly a support vector classifier. While all the most models performed quite decently, there were a few that stood out.

### K-Nearest Neighbors

```{r, echo = F, warning = F, message = F}
set.seed(445)
### KNN

cancer_10foldcv <- vfold_cv(train, v = 10)
neighbors_df <- data.frame(neighbors = c(1,3,5,7,10))

knn_spec <- nearest_neighbor(neighbors = tune("neighbors")) %>%
  set_engine("kknn") %>% 
  set_mode("classification") 

knn_rec <- recipe(diagnosis ~ ., data = train)

workflow() |>
  add_model(knn_spec) |>
  add_recipe(knn_rec) -> knn_wf

knn_wf |>
  tune_grid(resamples = cancer_10foldcv, grid = neighbors_df) -> knn_tune

knn_final <- finalize_workflow(knn_wf, select_best(knn_tune, metric = "accuracy"))
knn_final_fit <- fit(knn_final, data = train)

knn_final_fit |>
  augment(new_data = test) -> knn_test_res

knn_test_res |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  conf_mat(truth = diagnosis, estimate = pred_lower_cutoff) -> knn.cm

knn_test_res |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  accuracy(truth = diagnosis, estimate = pred_lower_cutoff) |>
  mutate(error = 1 - .estimate) |>
  pull(error) -> knn.err

100 - (round(knn.err, 4) * 100) -> knn.err

neighbors_best <- select_best(knn_tune, metric = "accuracy") |> pull(neighbors)
```

K-Nearest Neighbors is a classifying method that estimates the bayes classifier using the closest training points to the test point that is being classified. It uses a neighbor parameter that is a big factor in determining the classifier. For this model, we used 10-fold cross-validation to tune the neighbor parameter based on the highest accuracy. Once the neighbors parameter was tuned to `r neighbors_best`, the model was fit and prediction ensued. The confusion matrix showcasing the classification prediction of this model is below. 

```{r, echo = F, warning = F, message = F}
ggplot(data.frame(knn.cm$table), aes(x = Prediction, y = Truth, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 12) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "K-Nearest Neighbors Confusion Matrix", x = "Prediction", y = "Truth") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798") +
  guides(fill = F)
```

As shown in the confusion matrix, this KNN model only predicted two false-negatives out of 223 test observations and 90 true test malignant tumors. The KNN model predicted with an overall accuracy of `r knn.err`. While this model did perform quite well, we still want to improve these results.

### Support Vector Classifier

```{r, echo = F, warning = F, message = F}
set.seed(445)
### SVC

df_cost <- grid_regular(cost(), levels = 10)

svm_linear_tune_spec <- svm_poly(degree = 1, cost = tune("cost")) %>%
  set_mode("classification") %>%
  set_engine("kernlab", scaled = FALSE)

svm_linear_rec <- recipe(diagnosis ~ ., data = train)

svm_linear_wf <- workflow() |>
  add_model(svm_linear_tune_spec) |>
  add_recipe(svm_linear_rec)

tune_fit <- svm_linear_wf |>
  tune_grid(resamples = cancer_10foldcv, grid = df_cost)

svm_linear_final <- finalize_workflow(svm_linear_wf, select_best(tune_fit, metric = "accuracy"))

svm_linear_final |>
  fit(data = train) -> svm_linear_final_fit

svm_linear_final_fit |>
  extract_fit_engine() -> svm_linear_final_fit_engine

svm_linear_final_fit |>
  augment(new_data = test) |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  conf_mat(truth = diagnosis, estimate = pred_lower_cutoff) -> svm.cm

svm_linear_final_fit |>
  augment(new_data = test) |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  accuracy(truth = diagnosis, estimate = pred_lower_cutoff) |>
  mutate(error = 1 - .estimate) |>
  pull(error) -> svm.err

100 - (round(svm.err, 4) * 100) -> svm.err

cost_best <- select_best(tune_fit, metric = "accuracy") |> pull(cost)
```

A support vector classifier uses a softened margin classifier to classify the observations but with less sensitivity than a maximal margin classifier. The support vector classifier will not necessarily classify all training points accurately, but this allows for better test prediction. We tuned our support vector classifier to take a cost value of `r cost_best` using 10-fold cross-validation. The confusion matrix showcasing the prediciton accuracy of the support vector classifier model can be seen below.

```{r, echo = F, warning = F, message = F}
ggplot(data.frame(svm.cm$table), aes(x = Prediction, y = Truth, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 12) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Support Vector Classifier Confusion Matrix", x = "Prediction", y = "Truth") +
  theme_minimal() + 
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798") +  
  guides(fill = F)
```

As shown above, the support vector classifier predicted the exact same as the KNN model. With only 2 false negatives and an overall accuracy of `r svm.err`, this model was not an improvement from the KNN model, but it is reassuring to see consistent high accuracy amongst different models. The last model we will look at is a Random Forest model.

### Random Forest

```{r, echo = F, warning = F, message = F}
### Random Forest
set.seed(420)
rf_spec <- rand_forest(mtry = sqrt(.cols())) |>
  set_engine("randomForest", importance = TRUE) |>
  set_mode("classification")

rf_fit <- rf_spec |>
  fit(diagnosis ~ ., data = train)

rf_fit |>
  augment(new_data = test) |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  conf_mat(truth = diagnosis, estimate = pred_lower_cutoff) -> rf.cm

rf_fit |>
  augment(new_data = test) |>
  mutate(pred_lower_cutoff = factor(ifelse(.pred_M > 0.25, "M", "B"))) |>
  accuracy(truth = diagnosis, estimate = pred_lower_cutoff) |>
  mutate(error = 1 - .estimate) |>
  pull(error) -> rf.err

100 - (round(rf.err, 4) * 100) -> rf.err
```

Random Forest creates many decision trees from bootstrapped sample training points and uses a subset of predictors for each tree. The main difference between Random Forest and bagging decision trees is this feature subset that helps decorrelate the trees. For this Random Forest model, a default of 500 trees was used as tuning did not improve the model. The confusion matrix showcasing the prediction accuracy of the Random Forest model is shown below. 

```{r, echo = F, echo = F, warning = F, message = F}
ggplot(data.frame(rf.cm$table), aes(x = Prediction, y = Truth, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 12) +
  scale_fill_gradient(low = "white", high = "#c82798") +
  labs(title = "Random Forest Confusion Matrix", x = "Prediction", y = "Truth") +
  theme_minimal() +
  annotate("path",
   x=1+.25*cos(seq(0,2*pi,length.out=100)),
   y=2+.25*sin(seq(0,2*pi,length.out=100)), size=1.5, linetype="dashed", color="#c82798") +  
  guides(fill = F)
```

As shown above, the Random Forest model predicted the best out of all our models according to the standards we set. With only one false-negative, this is the lowest out of all the models and with an overall accuracy of `r rf.err` it holds up against the other models and does not gain too much Type I error. This confusion matrix portrays the predicition cutoff very well. Because the model biases towards classifying as malignant, there is much higher Type I error than Type II error, which is exactly what we were aiming for.

\newpage

# Ethans's Regression Proccess and Analysis

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Import packages
library(tidyverse)
library(ggplot2)
library(GGally)
library(knitr)
library(boot)
library(caret)
library(randomForest)
library(e1071)
library(class)
#
library(dplyr)
library(leaps)
library(caTools)
library(tidymodels)
library(glmnet)
library(forecast)
library(plyr)
library(readr)
library(repr)
library(kableExtra)

dataTemp <- read.csv("breast_cancer_diagnostic_data.csv")
datatemp2 <- dataTemp %>% select_if(~ !any(is.na(.)))
dataTot <- datatemp2[,!names(datatemp2) %in% c("id")]

testtemp <- read.csv("test.csv")
testTot <- testtemp[,!names(testtemp) %in% c("id")]

traintemp <- read.csv("training.csv")
trainTot <- traintemp[,!names(traintemp) %in% c("id")]
```

While Austin and Danny did Classification Analysis on `Type-II error rate`, I decided to do a Regression Analysis using the variable importance from their analysis's to compute the `training` and `test` error rate of those variables. 
This is to see how much error exists in a variable that is important in calculating if a cell is Malignant or Benign.
However, I will be focused on `test` error in my results as its more important to see how our results will be on future data rather than what we have already trained. The regression models that I will be using are:

    - Linear Regression
    - Ridge Regression
    - Lasso Regression
    
## Variables

In order to find the variables to run the Regression Analysis on, I had Austin and Danny run a variable importance plot on the data they collected and took the top 5 variables from each of their results, which are listed below.

- Austin Top 5 Importance Variables:

  1. *`area_mean`
  2. *`area_worst`
  3. *`concave.points_mean`
  4. `texture_mean`
  5. *`radius_worst`
    
- Danny Top 5 Importance Variables

  1. `concave.points_worst`
  2. *`concave.points_mean`
  3. *`area_worst`
  4. *`area_mean`
  5. *`radius_worst`

After I got both Austin's and Danny's top 5 importance variables, I noticed that there were 4 common variables in each of their results (designated with a '*' in the results above).
I then decided to see how the 4 common importance variables correlate to one another which can be seen in the pairwise scatterplot below:

```{r correlation_plot, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
corr_df <- dataTot[,c( 'area_worst', 'area_mean', 'concave.points_mean', 'radius_worst')]
ggpairs(corr_df) + labs(title = "Figure 1: Pairwise Scatterplot of Common Importance Variables")
```

What I found interesting was that `area_worst` & `raduis_worst` were more correlated that `area_worst` & `area_mean`. What I initially thought before making `Figure 1` was that `area_worst` & `area_mean` would be more correlated than the other 2 variables as they are both under the same variable category.

## area_mean

```{r, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
#Train
am_lm <- lm(area_mean ~ ., data = trainTot)
pred_amtrain<- predict(am_lm, trainTot)
amtrain_log <- postResample(pred_amtrain, trainTot$area_mean)
#amtrain_log
```
```{r, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
#Test
pred_amtest <- predict(am_lm, testTot)
amtest_log <- postResample(pred_amtest, testTot$area_mean)
#amtest_log
#summary(am_lm)
```

```{r, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
dummy <- dummyVars(area_mean~., data = dataTot[,-1])
x = as.matrix(predict(dummy, newdata = trainTot[,-1]))
y_train = trainTot$area_mean

x_test = as.matrix(predict(dummy, newdata = testTot[,-1]))
y_test = testTot$area_mean

lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, alpha = 0, family = 'gaussian', lambda = lambdas)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min

eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquared = R_square)
  
}

# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
amr_train <- eval_results(y_train, predictions_train, trainTot)

# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
amr_test <- eval_results(y_test, predictions_test, testTot)

#print(amr_train)
#print(amr_test)
```

```{r, results=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
lambdas <- 10^seq(2, -3, by = -.1)
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 10)

lambda_best <- lasso_reg$lambda.min 
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
aml_train <- eval_results(y_train, predictions_train, trainTot)

predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
aml_test <- eval_results(y_test, predictions_test, testTot)

#print(aml_train)
#print(aml_test)
```

For `area_mean` after running the 3 regression models, I found the lowest `test error` is from the ridge regression model with an `RMSE` of `23.70795` with a `Rsquared` of `99.489%`. 
This mean that using ridge regression, we can explain `99.489%` of the variance in `area_mean` in Austin's and Danny's models.
The ridge regression corresponding `training error` is `17.87197` with a `Rsquared` of 	`99.759%`.
This corresponds to that `99.759%` of our variance in `area_mean` was trained in Austin's and Danny's models.

```{r area_mean_table, echo=FALSE, message=FALSE, warning=FALSE}
table_am <- as_tibble(rbind(amtrain_log,
      amtest_log,
      amr_train,
      amr_test,
      aml_train,
      aml_test)) %>%
    mutate(model = c('Linear Train Error',  'Linear Test Error','Ridge Train Error', 'Ridge Test Error', 'Lasso Train Error', 'Lasso Test Error')) %>%
    select(model, RMSE, Rsquared)
table_am %>% 
  kbl(digits = 5 ) %>% 
  kable_styling(bootstrap_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position")
```

## Area_worst

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
#Test
aw_lm <- lm(area_worst ~ ., data = trainTot)
pred_awtest <- predict(aw_lm, testTot)
awtest_log <- postResample(pred_awtest, testTot$area_worst)
#awtest_log
#summary(am_lm)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
#Train
pred_awtrain<- predict(aw_lm, trainTot)
awtrain_log <- postResample(pred_awtrain, trainTot$area_worst)
#awtrain_log
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
dummy <- dummyVars(area_worst~., data = dataTot[,-1])
x = as.matrix(predict(dummy, newdata = trainTot[,-1]))
y_train = trainTot$area_worst

x_test = as.matrix(predict(dummy, newdata = testTot[,-1]))
y_test = testTot$area_worst

lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, alpha = 0, family = 'gaussian', lambda = lambdas)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min

eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquared = R_square)
  
}

# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
awr_train <- eval_results(y_train, predictions_train, trainTot)

# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
awr_test <- eval_results(y_test, predictions_test, testTot)

#print(awr_train)
#print(awr_test)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
lambdas <- 10^seq(2, -3, by = -.1)
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 10)

lambda_best <- lasso_reg$lambda.min 
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
awl_train <- eval_results(y_train, predictions_train, trainTot)

predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
awl_test <- eval_results(y_test, predictions_test, testTot)

#print(awl_train)
#print(awl_test)
```

In `area_worst` after running the 3 regression models, I found the lowest `test error` is from the ridge regression model with an `RMSE` of `42.28132` with a `Rsquared` of `99.288%`. 
This mean that using ridge regression, we can explain `99.288%` of the variance in `area_worst` in Austin's and Danny's models.
The ridge regression corresponding `training error` is `27.36780` with a `Rsquared` of `99.799%`.
This corresponds to that `99.799%` of our variance in `area_worst` was trained in Austin's and Danny's models.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.pos = "H", out.extra = ""}
table_aw <- as_tibble(rbind(awtrain_log,
      awtest_log,
      awr_train,
      awr_test,
      awl_train,
      awl_test)) %>%
    mutate(model = c('Linear Train Error',  'Linear Test Error','Ridge Train Error', 'Ridge Test Error', 'Lasso Train Error', 'Lasso Test Error')) %>%
    select(model, RMSE, Rsquared)
table_aw %>% 
  kbl(digits = 5 ) %>% 
  kable_styling(bootstrap_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position")
```

## concave.points_mean

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
#Train
cvptsm_lm <- lm(concave.points_mean ~ ., data = trainTot)
pred_cvptsm_train<- predict(cvptsm_lm, trainTot)
cvptsm_train_log <- postResample(pred_cvptsm_train, trainTot$concave.points_mean)
#cvptsm_train_log
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
#Test
pred_cvptsm_test <- predict(cvptsm_lm, testTot)
cvptsm_test_log <- postResample(pred_cvptsm_test, testTot$concave.points_mean)
#cvptsm_test_log
#summary(am_lm)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE, fig.pos = "H", out.extra = ""}
dummy <- dummyVars(concave.points_mean~., data = dataTot[,-1])
x = as.matrix(predict(dummy, newdata = trainTot[,-1]))
y_train = trainTot$concave.points_mean

x_test = as.matrix(predict(dummy, newdata = testTot[,-1]))
y_test = testTot$concave.points_mean

lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, alpha = 0, family = 'gaussian', lambda = lambdas)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min

eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquared = R_square)
  
}

# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
cvptsmr_train <- eval_results(y_train, predictions_train, trainTot)

# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
cvptsmr_test <- eval_results(y_test, predictions_test, testTot)

#print(cvptsmr_train)
#print(cvptsmr_test)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
lambdas <- 10^seq(2, -3, by = -.1)
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 10)

lambda_best <- lasso_reg$lambda.min 
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
cvptsml_train <- eval_results(y_train, predictions_train, trainTot)

predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
cvptsml_test <- eval_results(y_test, predictions_test, testTot)

#print(cvptsml_train)
#print(cvptsml_test)
```

With `concave.points_mean` after running the 3 regression models, I found the lowest `test error` is from the linear regression model with an `RMSE` of `0.00574` with a `Rsquared` of `97.847%`. 
This mean that using ridge regression, we can explain `97.847%` of the variance in `concave.points_mean` in Austin's and Danny's models.
The ridge regression corresponding `training error` is `0.00467` with a `Rsquared` of `98.532%`.
This corresponds to that `98.532%` of our variance in `concave.points_mean` was trained in Austin's and Danny's models.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.pos = "H", out.extra = ""}
table_am <- as_tibble(rbind(cvptsm_train_log,
      cvptsm_test_log,
      cvptsmr_train,
      cvptsmr_test,
      cvptsml_train,
      cvptsml_test)) %>%
    mutate(model = c('Linear Train Error',  'Linear Test Error','Ridge Train Error', 'Ridge Test Error', 'Lasso Train Error', 'Lasso Test Error')) %>%
    select(model, RMSE, Rsquared)
table_am %>% 
  kbl(digits = 5 ) %>% 
  kable_styling(bootstrap_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position")
```

## radius_worst

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
#Train
rw_lm <- lm(radius_worst ~ ., data = trainTot)
pred_rw_train<- predict(rw_lm, trainTot)
rw_train_log <- postResample(pred_rw_train, trainTot$radius_worst)
#rw_train_log
```
```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
#Test
pred_rw_test <- predict(rw_lm, testTot)
rw_test_log <- postResample(pred_rw_test, testTot$radius_worst)
#rw_test_log
#summary(am_lm)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE, fig.pos = "H", out.extra = ""}
dummy <- dummyVars(radius_worst~., data = dataTot[,-1])
x = as.matrix(predict(dummy, newdata = trainTot[,-1]))
y_train = trainTot$radius_worst

x_test = as.matrix(predict(dummy, newdata = testTot[,-1]))
y_test = testTot$radius_worst

lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y_train, alpha = 0, family = 'gaussian', lambda = lambdas)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min

eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquared = R_square)
  
}

# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
rwr_train <- eval_results(y_train, predictions_train, trainTot)

# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
rwr_test <- eval_results(y_test, predictions_test, testTot)

#print(rwr_train)
#print(rwr_test)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
lambdas <- 10^seq(2, -3, by = -.1)
lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 10)

lambda_best <- lasso_reg$lambda.min 
lasso_model <- glmnet(x, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
rwl_train <- eval_results(y_train, predictions_train, trainTot)

predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
rwl_test <- eval_results(y_test, predictions_test, testTot)

#print(rwl_train)
#print(rwl_test)
```

For `radius_worst` after running the 3 regression models, I found the lowest `test error` is from the lasso regression model with an `RMSE` of `0.24120	` with a `Rsquared` of `99.713%`. 
This mean that using ridge regression, we can explain `99.713%` of the variance in `radius_worst` in Austin's and Danny's models.
The ridge regression corresponding `training error` is `0.15907` with a `Rsquared` of `99.900%`.
This corresponds to that `99.900%` of our variance in `radius_worst` was trained in Austin's and Danny's models.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.pos = "H", out.extra = ""}
table_am <- as_tibble(rbind(rw_train_log,
      rw_test_log,
      rwr_train,
      rwr_test,
      rwl_train,
      rwl_test)) %>%
    mutate(model = c('Linear Train Error',  'Linear Test Error','Ridge Train Error', 'Ridge Test Error', 'Lasso Train Error', 'Lasso Test Error')) %>%
    select(model, RMSE, Rsquared)
table_am %>% 
  kbl(digits = 5 ) %>% 
  kable_styling(bootstrap_options = "striped") %>%
  kable_styling(latex_options = "HOLD_position")
```

\newpage

# Analysis Summary

From the results of our classification analysis, we are confident not only in our predictions of tumor diagnosis, but reducing the amount of false-negative diagnoses. Through our methods of tuning specifically for Type II error, and lowering the prediction cutoff for malignant tumors, we have shown two successful ways of battling classification data with a preffered error class. This is extremely important in cancer diagnosis, as well as other analyses, as we do not want to tell someone they don't have cancer when they do, as that could delay treatment and further risk their life. 

In terms of feature importance and feature selection, there were many limitations in interpretations due to the correlation of the measurement types. We did find that there were a number of the same features outlined by different models, so we can conclude that the variables, regardless of the measurement type, hold importance in classifying a tumor diagnosis.

# Potential Improvements

Following our analysis, we have identified a few potential improvements that could be made to our analysis to further improve our models.
The obvious and first improvement that could be made is to **collect more data**. With more data, we would allow our models to see more examples of malignant cells and be able to better classify them.
As you saw in the Exploratory Data Analysis section, we have more more information regarding the benign cells than the malignant cells. 
If we had more data, we could balance out the data set and have more information regarding the malignant cells.
Another possible improvement is to **collect different or more features** regarding each cell. 
Most of the features included the mean, worst and standard error of geometry measurements of the cell. This leads to many variables being highly correlated with each other and could lead to multicollinearity.
If we had more features, we could reduce the multicollinearity and diversify the information we have regarding each cell.
Finally, if we had **access to better technology** and more time or money, we could use a more advanced machine learning model.
Because this project was done for a class, we were limited to the models we could use as there was a time contraint, especially in a team enviroment.

\newpage

# Works Cited

- Breast Cancer Wisconsin (Diagnostic) Data Set:
    - https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data?datasetId=180&sortBy=voteCount
- Definition of Features (Variables):
    - https://www.causeweb.org/usproc/sites/default/files/usclap/2017-2/Evaluating_Benign_and_Malignant_Breast_Cancer_Cells_from_Fine-Needle_Aspirates.pdf